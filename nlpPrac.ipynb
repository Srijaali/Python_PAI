{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI83w2rmHqZ_",
        "outputId": "c5ffd149-ec1c-45d4-edec-ffde7063fbe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization:  ['Hi', '.', 'My', 'name', 'is', 'Rija', 'Ali', '.']\n",
            "Sent tokenize ['Hi.', 'My name is Rija Ali.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "text = \"Hi. My name is Rija Ali.\"\n",
        "words = word_tokenize(text)\n",
        "print(\"Word Tokenization: \" , words)\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sent tokenize\", sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = ['jumpin','jumps','jumped','running','ran','easily']\n",
        "stemmed_words =[ stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"original: \", words)\n",
        "print(\"stemmed: \",stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1OZekGARd6x",
        "outputId": "21cc3be6-764a-4f23-9916-7c036e7873cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original:  ['jumpin', 'jumps', 'jumped', 'running', 'ran', 'easily']\n",
            "stemmed:  ['jumpin', 'jump', 'jump', 'run', 'ran', 'easili']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['jumpin','jumps','jumped','running','ran','easily']\n",
        "lem_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"ori: \",words)\n",
        "print(\"lemma: \" , lem_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA27PTvrR1su",
        "outputId": "bf3ecd29-84be-432f-ca83-e8c8da5c8d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ori:  ['jumpin', 'jumps', 'jumped', 'running', 'ran', 'easily']\n",
            "lemma:  ['jumpin', 'jump', 'jumped', 'running', 'ran', 'easily']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"This is a smalple text.\"\n",
        "words = word_tokenize(text)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filter_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "print(\"ori: \" , words)\n",
        "print(\"stopped: \" , filter_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGDgy3xJcWOk",
        "outputId": "2ba2ed03-282c-4973-a5e1-f7dc6dcb6ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ori:  ['This', 'is', 'a', 'smalple', 'text', '.']\n",
            "stopped:  ['smalple', 'text', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pos tagging : parts of speech\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# # Input text\n",
        "# text = 'steeming helps in word tokenization'\n",
        "\n",
        "# # Tokenize and POS tag\n",
        "# toki = word_tokenize(text)\n",
        "# posooo = pos_tag(toki)\n",
        "\n",
        "# # Print the result\n",
        "# print(posooo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emgxjRIGViKg",
        "outputId": "e9eb51ec-a592-4036-ef48-1d5ff433dcfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}